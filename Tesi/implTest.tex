\chapter{Implementazione e test}
\label{chap:implTest}

\begin{minipage}{12cm}\textit{In questo capitolo si illustreranno gli aspetti tecnici relativi alla realizzazione del software: la scelta delle librerie e dell'ambiente di sviluppo, i problemi riscontrati e le soluzioni intraprese. Si procederà inoltre alla validazione del sistema e la valutazione delle prestazioni.}
\end{minipage}

\vspace*{1cm}

\section{Lo sviluppo}
\label{sec:sviluppo}
Nei precedenti capitoli si è discusso principalmente degli strumenti teorici che, come spesso segnalato, sono stati selezionati o sintetizzati con in mente il requisito di efficienza imprescindibile in un contesto di controllo nel quale ci si pone. Sono stati selezionati perciò gli algoritmi migliori rispetto ad un rapporto efficacia/costo computazionale. Inoltre, si sono scelti quelli meglio parallelizzabili su architetture vettorizzate o dotate di co-processori grafici. Nel seguito verrà descritto l'aspetto implementativo del software Visual Observer. Si descriveranno in particolare gli strumenti di sviluppo, le librerie usate e l'architettura software implementata. 

\subsection{Ambiente di sviluppo e target}
\label{sec:dev:ambiente}

Prima di poter scrivere anche una sola linea di codice, si deve aver ben chiaro l'obbiettivo di sviluppo che ci si pone al fine di poter valutare quale può essere l'ambiente di sviluppo, il linguaggio e le librerie da utilizzare.  Conviene allora specificare e definire i requisiti di progetto:
\begin{itemize}
	\item \textbf{Architettura target:}  x86\_64, ARM,
	\item \textbf{Sistema Operativo target:}  Windows, GNU/Linux,
	\item \textbf{Specifiche hardware minime:}
	\begin{itemize}
		\item CPU: x86\_64 o ARM,
		\item GPU: nessuna o generica con supporto OpenCL,
	\end{itemize} 
	\item \textbf{Specifiche hardware consigliate:}
		\begin{itemize}
			\item CPU: x86\_64 o ARM con supporto OpenCL o SIMD/AVX,
			\item GPU: coprocessore grafico Nvidia ad alte prestazioni con supporto CUDA Shared Model $\ge$ 2.1,
		\end{itemize}			 
\end{itemize}

La scelta delle architetture e dei sistemi operativi non è casuale, ovviamente. Infatti, il supporto x86\_64 permette di poter sviluppare e testare il codice su un normale PC in ambiente Windows o Linux. D'altro canto il software è pensato per essere utilizzato a bordo di un sistema di controllo di un robot e tipicamente quest'ultimi hanno processori ARM su sistema Linux.

Spesso si è fatto riferimento nei capitoli precedenti ad un requisito di esecuzione real-time. Formalmente esso è difficile da definire, si può intendere però nel seguente modo: \textit{si fissi una piattaforma hardware abbastanza potente, si richiede che il tempo di esecuzione del software di stima sia tale da rendere la stessa disponibile in tempo utile.} Ad esempio, su sistemi moderni si potrebbe richiedere che sia in grado si fornire una stima con frequenza 20 Hz o superiore.
 
Si può comprendere allora la specifica richiesta della presenza di un processore vettorizzato o un coprocessore grafico con supporto a OpenCL o CUDA. Infatti, in questi casi è possibile ridurre di moltissimo i tempi di esecuzione grazie alla parallelizzazione dei calcoli.

Alla luce di quanto su detto, la scelta del linguaggio di sviluppo è ricaduta sul \textbf{C++}. Infatti esso permette di ottenere un codice macchina particolarmente ottimizzato e al contempo di mantenere la portabilità dello stesso. Il codice è stato sviluppato utilizzando il sistema operativo \textbf{Windows} e l'ambiente di sviluppo \textbf{Visual Studio Community 2015}. Tale scelta è motivata da diversi fattori:
\begin{enumerate}
	\item Allo stato attuale, Visual Studio permette di sviluppare e compilare codice, in modo semplice, per tutti i sistemi operativi e le architetture su citati.
	\item In ambiente Linux non si sarebbe potuto sviluppare per Windows.
	\item Rende lo sviluppo più semplice e veloce, grazie anche a numerosi strumenti di debug e profiling messi a disposizione.
	\item Per scopi accademici (e piccoli progetti commerciali) è distribuito gratuitamente.
\end{enumerate}

Nel seguito si presenta una breve descrizione delle librerie software utilizzate: OpenCV e CUDA. Non si farà riferimento a OpenCL perché non direttamente utilizzata. Infatti, grazie all'uso di OpenCV l'utilizzo della stessa o di un altro sistema di vettorizzazione è del tutto trasparente. Come sarà chiaro più avanti nel paragrafo dedicato al testing, l'attivazione o meno di quest'ultima però darà luogo a prestazioni nettamente superiori. Informalmente, OpenCL è un architettura hardware e software per il calcolo parallelo. Essa permette di utilizzare sia CPU che GPU (anche Nvidia), talvolta in contemporanea, per effettuare calcoli generici.

\subsection{OpenCV}
\label{sec:tools:opencv}

OpenCV, acronimo in lingua inglese di \textbf{Open} source \textbf{C}omputer \textbf{V}ision library, è una libreria software multi piattaforma per la visione artificiale. Essa nella sua prima versione risale al lontano 2000 ed è stata originariamente sviluppata ad opera di Intel. Successivamente il progetto è stato preso in carico dalla comunità open source ed è mantenuto da un numero elevato di appassionati e sviluppatori. Oggi giorno, è considerata lo standard di fatto utilizzato per progetti comprendenti la visione artificiale in progetti sia accademici che commerciali. Essa comprende molteplici ambiti tra cui: calcolo matriciale e strumenti di algebra lineare, riconoscimento delle feature, stereoscopia e filtraggio delle immagini. Di giorno in giorno gli strumenti presenti vengono ottimizzati e di nuovi ne vengono aggiunti. 
La libreria è scritta principalmente in codice C++, fortemente ottimizzato per sfruttare le particolarità della piattaforma target: vettorizzazione SIMD e AVX, openCL, CUDA. Risulta perciò un valido strumento in progetti di applicazioni real-time. Esistono poi numerosi wrapper che permettono il suo impiego in altri linguaggi tra cui: Python, Java, C\#. Inoltre è disponibile per praticamente tutti i maggiori sistemi operativi: Windows, Linux, Android, IOs e OsX e per le maggiori architetture: x86, x86\_64, ARM.
Infine si segnala che è rilasciata sotto licenza BSD perciò può essere liberamente impiagata in progetti sia accademici che commerciali.

\subsection{CUDA}
\label{sec:tools:cuda}
CUDA (acronimo di Compute Unified Device Architecture) è un'architettura hardware per l'elaborazione parallela creata da NVIDIA. Essa, unitamente all'ambiente di sviluppo CUDA, permette scrivere applicazioni capaci di eseguire calcolo parallelo sulle GPU Nvidia. I linguaggi di programmazione disponibili nell'ambiente di sviluppo per CUDA, sono estensioni dei linguaggi più diffusi per scrivere programmi. Il principale è "CUDA-C" (C con estensioni NVIDIA), altri sono estensioni di Python, Fortran, Java e MATLAB.

Diversamente dalle CPU, le GPU hanno un'architettura parallela con diversi core, ognuno capace di eseguire centinaia di processi simultaneamente, se un'applicazione è adatta per quel tipo di architettura la GPU può offrire grandi prestazioni e benefici. Questo approccio di risoluzione dei problemi è noto come GPGPU (General Purpose GPU). In generale un ottimo contesto di esecuzione per questo tipo di dispositivi è quando vengono avviati moltissimi thread con lo stesso flusso di esecuzione. In realtà infatti le GPU estremizzano il concetto di vettorizzazione. Si hanno a disposizione un certo numero di Shared Processor, capaci di eseguire un gruppo di thread alla volta detto Warp. Per poter eseguire un Warp in modo parallelo deve accadere che tutti i thread che lo compongono eseguano le stesse operazioni, ovviamente su indirizzi di memoria differenti. Inoltre per rendere gli accessi in memoria efficienti e ridurre (anche drasticamente) i tempi di calcolo i thread devono poter accedere alla memoria in modo adatto. Infatti, le GPU gestiscono la memoria in modo opposto rispetto alle CPU, cioè locazioni di memoria vicine sono disposte in diversi banchi di memoria. Il motivo di tale scelta risiede nel fatto che le richieste in memoria sono solitamente molto lente e si cerca di parallelizzare la lettura in memoria al fine di rendere i dati disponibili simultaneamente per tutti i thread di un Warp. 

Alla luce di ciò, è chiaro che non tutte le applicazioni possono essere parallelizzate su coprocessore grafico ed è inoltre chiaro perché un forte accento si è posto sul trovare algoritmi parallelizzabili. Ad esempio questo è il caso del comparatore a forza bruta, presentato nel paragrafo \ref{sec:det:bmmatch}, che seppur utilizza un approccio poco intelligente risulta molto veloce.

\subsection{L'architettura software}
\label{sec:dev:arch}
Infine, per concludere il paragrafo, si presenta l'architettura software sviluppata. Essa è pensata per essere completamente modulare ed intercambiabile. I blocchi funzionali contraddistinti con la lettera "I" maiuscola sono delle interfacce e diverse implementazioni ne sono fornite.
\begin{figure}[h!]
	\centering
	\includegraphics[width=400pt]{imgs/arch.png}
	\caption{Rappresentazione architettura software Visual Observer.}
	\label{vis:impl:arch}
\end{figure} 

L'architettura inoltre prevede che essi possano essere sostituiti da future implementazioni. Con riferimento alla figura \ref{vis:impl:arch} è possibile verificare che il sistema Visual Observer si compone di tre principali blocchi funzionali: 
\begin{enumerate}
	\item \textbf{Camera Manager}: ha lo scopo di gestire le camere e astrarre l'acquisizione dell'input visuale verso il Visual Observer. Esso può inoltre gestire più camere anche di diverso tipo. Le camere possono essere registrate attraverso l'interfaccia \textbf{ICamera} e un identificativo univoco. In tal modo è possibile richiedere al manager di acquisire un immagine da un determinato sensore. Si è prevista inoltre la possibilità di richiedere l'input sincrono da parte di due sensori grafici allo scopo di ottenere una coppia di immagini stereo catturate nello stesso istante.
	L'interfaccia ICamera astrae il manager dal funzionamento del sensore stesso e permette quindi l'utilizzo di sensori di diverso tipo senza dover modificare il codice a monte. Tale interfaccia espone diversi metodi tra cui: calibrazione, acquisizione grezza e acquisizione rettificata. Il primo metodo è pensato per eseguire la calibrazione del sensore e quindi la stima dei parametri di distorsione cui si è accennato nel capitolo \ref{chap:visione}. Il secondo ed il terzo pensati per richiedere la cattura di un immagine o di una rettificata rispettivamente, utilizzando i parametri ottenuti in fase di calibrazione. Per gestire una camera è necessario quindi implementare tale interfaccia e scrivere il codice necessario solo alla gestione del sensore che si vuole utilizzare. Potrebbe essere quindi necessario, a seconda del sensore, dover scrivere un driver o semplicemente dialogare con uno già esistente. In figura \ref{vis:impl:arch} ad esempio sono stati collegati due sensori USB.
	
	\item \textbf{IEstimator}: è un astrazione dell'algoritmo di stima della matrice di trasformazione che si vuole utilizzare. Espone a monte solo i metodi atti all'introduzione dell'input, richiesta della stima ed estrazione dell'output. Si è previsto quindi di poter cambiare, anche durante l'esecuzione, l'algoritmo usato. Questo meccanismo risulta molto utile soprattutto in fase di test, è possibile sperimentare i risultati ottenuti utilizzando diversi algoritmi. Inoltre, in una futura estensione si potrebbe prescindere dagli algoritmi implementati ed utilizzarne degli altri senza apportare ancora una volta nessuna modifica a monte.
	
	Si sono quindi implementati i diversi algoritmi di stima presentati nel capitolo \ref{chap:stima} ed è possibile scegliere quale utilizzare. Per l'algoritmo di Kabsch ad esempio si è sperimentato con diverse implementazioni, una su CPU e un altra che fa uso del processore grafico.
	
	\item \textbf{ISetsGenerator}: questo blocco funzionale è adibito alla generazione dei set di coordinate dei punti omologhi cui si è discusso nel capitolo \ref{chap:visione}. Accetta in ingresso l'input visivo e restituisce a monte i set suddetti senza esibire nulla riguardo il suo funzionamento interno. In figura \ref{vis:impl:arch} si può evincere che ad esso è collegato un ulteriore blocco funzionale IPairsGenerator. In effetti questo tipo di collegamento non è obbligatorio, diverse implementazioni potrebbero anche farne a meno ma si è preferito segnalarne il possibile collegamento, dato che in effetti nell'implementazione attuale viene utilizzato. Con ordine quindi:
	\begin{itemize}
		\item \textbf{IPairsGenerator}: è un astrazione dell'algoritmo adibito alla ricerca di corrispondenze in due immagini. Concettualmente, accetta due immagini in ingresso e fornisce le corrispondenze tra i punti chiave trovati. Si è implementato quanto detto nel capitolo \ref{chap:visione}: FAST + BRIEF + BF matcher. Si sono previste, scrivendo codice OpenCV, implementazioni che fanno uso sia della vettorizzazione su CPU e/o utilizzano OpenCL. Inoltre per i sistemi dotati di processore grafico Nvidia si è previsto l'utilizzo di un implementazione CUDA.
		\item Inoltre si è implementato un ISetsGenerator che, in concordanza a quanto detto nel paragrafo \ref{sec:vision:solution}, fa uso degli IPairsGenerator forniti al fine di generare i suddetti set.
	\end{itemize}
	Tutto il codice implementato risulta modulare e può essere modificato o sostituito senza necessità di riscrivere il codice di gestione. Visual Observer può essere stanziato specificando quale implementazione utilizzare a seconda delle necessità o delle disponibilità hardware. Per quanto riguarda il funzionamento ad alto livello esso si limita ad utilizzare i livelli sottostanti senza necessità di conoscerne il funzionamento, dimostrando così il potenziale che un architettura a livelli di astrazione può fornire. In particolare, si utilizza il manager per richiedere un nuovo input dall'esterno dai sensori frontali o da una qualsiasi altra coppia sensori. L'input ottenuto viene fornito poi all'ISetsGenerator che restituisce i due set di coordinate. Quest'ultimi vengono infine passati all'IEstimator per ottenere la stima vera e propria che può essere poi fornita a monte ad un qualsivoglia controllore o osservatore.
	 
\end{enumerate}
\section{Validazione e test}
\label{sec:test}
Infine per concludere il capitolo si presentano dei test effettuati, atti allo scopo di validare i risultati ottenuti ed inoltre valutare le performance di esecuzione. I test sono stati effettuati sulla seguente macchina:
\begin{itemize}
	\item \textbf{CPU:} Intel i7-6700HQ @ 2.60 GHz,
	\begin{itemize}
		\item \textbf{Architettura:} x86\_64, 
		\item \textbf{Specifiche:} AVX, AVX2, OpenCL,
	\end{itemize}		
	\item \textbf{Memoria:} 16 GB, 
	\item \textbf{GPU:} Nvidia Geforce GTX 1070,
	\begin{itemize}
		\item \textbf{Memoria dedicata:} 8 GB, 
		\item \textbf{Specifiche:} OpenCL, CUDA SM 6.1
	\end{itemize}			
	\item \textbf{Sistema Operativo:}  Windows.	 
\end{itemize}

Al fine di poter effettuare una validazione dei risultati, è necessario poter essere in grado di confrontare i risultati con una misura reale. Inoltre, si deve poter disporre di un ambiente di test adibito e di un prototipo. Sebbene le ultime due, a meno di ulteriore impiego di tempo, siano superabili, la prima risulta più ostica. Infatti la misura di controprova stessa sicuramente sarebbe affetta da un forte errore. Si é scelto, a valle di tutti questi fattori, di optare per un test in \textbf{ambiente virtuale} simulato. Al fine di ciò si è utilizzato un motore grafico, Unity3D. 
\subsection{Il motore grafico Unity3D}
\label{sec:unity}

Unity è un motore grafico cross-platform sviluppato da Unity Technologies, che viene utilizzato principalmente per sviluppare videogiochi e simulazioni per computer, console e dispositivi mobili. Unity supporta grafica 2D e 3D, ha un suo editor di sviluppo e permette lo scripting tramite C\# o altri linguaggi. Esso utilizza le seguenti API grafiche: Direct3D su Windows e Xbox One; OpenGL su Linux, MacOS e Windows; OpenGL ES su Android e iOS; WebGL sul web; e API proprietarie sulle console di videogiochi.
Unity consente di specificare le impostazioni grafiche per ogni piattaforma supportata e fornisce il supporto per riflessioni, mappatura parallasse, occlusione dello spazio dello schermo (SSAO), ombre statiche e dinamiche, rendering e gli effetti post-elaborazione a schermo intero. 
Unity offre anche servizi agli sviluppatori come: Unity Ads, Unity Analytics, Unity Cloud Build, Unity Everyplay, Unity IAP, Unity Multiplayer, Unity Performance Reporting e Unity Collaborate. Che permettono l'integrazione di pubblicità nei propri prodotti, velocizzare lo sviluppo e creare facilmente prodotti di rete.


\subsection{Validazione con motore grafico}
\label{sec:valid}
Utilizzando il sudetto Unity3d si è proceduto alla creazione di uno scenario. Al fine di dimostrale le abilità del sistema, si è scelto uno scenario che presentasse caratteristiche scomode. Si è optato, anche in base ai modelli e texture disponibili gratuitamente, per svolgere i test all'interno di un astronave. Lo scenario dispone delle seguenti caratteristiche:
\begin{enumerate}
	\item presenta molte parti della scena simili e che potrebbero essere confuse,
	\item sono presenti effetti di luce, ombre, riflessioni e fumo che sono tipicamente considerati problematici per la tipologia di strumento in esame,
	\item in alcune zone della scena sono stati posti oggetti animati che per potrebbero comportare, per via del funzionamento attuale dello stimatore che suppone un ambiente circostante statico, un errore sulla stima.
\end{enumerate}

Si è quindi proceduto a creare un prototipo virtuale, cioè una coppia di camere solidali ad un supporto virtuale. Per effettuare il test vero e proprio, è stato adibito uno script che permettesse la cattura simultanea dalle due camere. Le immagini vengono poi memorizzate insieme all'informazione di posizione ed orientamento del supporto che fa le veci del robot mobile.

Si è dovuto poi fissare i parametri $T = 0.4$ Unità Unity ed $f = 581.6$ visti nel capitolo \ref{chap:visione}. In particolare il secondo potrebbe non sembrare un valore tipico, infatti ad esempio una tipica lente reale ha una focale di lunghezza pari a 35 millimetri o poco più. Quest'ultimo valore deriva dal fatto che il motore grafico Unity non simula una camera reale ma semplicemente si limita ad effettuare delle proiezioni sul piano di cattura al fine di essere più veloce. Il valore è stato individuato mediante delle misure virtuali e stimato. Infatti invertendo la formula \ref{eq:stimaZ} si può ricavare il valore di f conoscendo $T$, la disparità $d$ associata al punto e la sua distanza reale dal punto di osservazione. Trattandosi di un ambiente virtuale quest'ultima informazione è facile da reperire. Nel caso reale, si effettua un procedimento simile effettuando delle vere misurazioni.  


In figura \ref{vis:test:scena} è possibile ammirare qualche istantanea catturate all'interno dell'ambiente virtuale.
\begin{figure}[h!]
	\centering
	\includegraphics[width=440pt]{imgs/scene.png}
	\caption{Alcune istantanee catturate all'interno dell'ambiente virtuale.}
	\label{vis:test:scena}
\end{figure} 

\subsection{Risultati ottenuti}
\label{sec:perf}
Sono stati effettuati diversi test: sia dei singoli componenti di stima che del sistema nella sua totalità. Questo è quanto è emerso:

\begin{itemize}
	\item Algoritmi di stima: sono stati effettuati test introducendo un errore normale sulle misure con valore atteso nullo e devianza tra i $10^{-3} \div 10^{-1}$.
	\begin{itemize}
		\item Il metodo di Groebner: sebbene teoricamente valido ed istruttivo, esso non presenta una grande robustezza rispetto ai disturbi. Infatti, nessuna teoria è stata sviluppata a riguardo. Tipicamente l'errore di misura viene riportato sulla stima stessa e talvolta amplificato. La stima può essere migliorata ripetendo il calcolo più volte su coppie diverse e facendo la media campionaria del risultato.
		\item Il metodo di Kabsch: i risultati ottenuti sono stati molto promettenti, infatti tipicamente l'errore di stima commesso veniva ridotto anche di diversi ordini di grandezza già a partire da un numero di punti esiguo pari a $10 \div 30$. Si è riscontrato un comportamento in accordo con quanto teoricamente proposto. Utilizzando un numero di punti piuttosto grande come 10000 si riesce ad ottenere una stima pressoché "perfetta".
	\end{itemize}
	\item Algoritmi di feature detection: in generale la stima delle feauture e il conseguente calcolo della disparità dei punti chiave si è rivelato più che soddisfacente. Si può affermare che all'aumentare della risoluzione dei sensori utilizzati si nota un miglioramento del risultato finale, ovviamente a discapito delle prestazioni. In accordo alle formule trovate, da notare che nel calcolo della coordinata Z c'è una divisione per d (la disparità), si è riscontrato un comportamento rispetto al quale la stima della distanza per i punti più lontani risulta meno accurata. Infatti se la disparità è piccola, l'errore di pochi pixel si traduce in distanze calcolate piuttosto differenti. Una stima errata della linea di base $T$ o della lunghezza focale della camera $f$ invece introduce un errore sistematico moltiplicativo sulla stima. Tipicamente però la conoscenza accurata di quest'ultimi è facile da ottenere.
	
	\item Visual Observer: per il sistema completo i risultati ottenuti sono stati anche in questo caso in linea a quanto sperato. Infatti con immagini ad una risoluzione di 640x480 si è ottenuto nella maggior parte dei casi una stima della posizione con precisione dell'ordine dei centesimi di unità Unity mentre per la matrice di rotazione un errore quadratico intorno ai 10 millesimi. Il fatto che l'orientamento venga stimato in modo migliore è giustificabile dal fatto che nella stima della matrice di rotazione un eventuale errore sistematico sulla stima delle distanze è del tutto irrilevante. Per quanto riguarda il tempo di esecuzione, utilizzando la macchina sopra scritta, esso si attesta a circa 30 ms sia in modalità OpenCL che CUDA. Quindi con una frequenza di circa 30 Hz. Se si disattivano le ottimizzazioni invece il tempo di esecuzione peggiora anche di due ordini di grandezza. Sebbene ci si attendevano prestazioni migliori di CUDA rispetto all'implentazione OpenCL, va detto che potrebbe essere dovuto ad una minore ottimizzazione effettuata nei metodi CUDA di openCV che sono stati utilizzati. Inoltre si sta utilizzando una scheda piuttosto recente nella quale quasi sicuramente è stata resa più performante rispetto al supporto ad OpenCL. 
	La maggior parte dell'errore di stima è sicuramente imputabile al sistema di visione, alla bassa risoluzione delle immagini ed al fatto che troppo spesso i punti chiave considerati, nello scenario in esame, risultavano lontani. Le riflessioni, il fumo e le luci non sembrano invece comportare un grosso errore ma piuttosto limitano il numero di punti chiave individuati. Per quanto riguarda gli oggetti in movimento, ovviamente, essi in diversi casi introducono un errore. Si è notato però che qualora essi siano piuttosto uniformi e identificabili dai soli bordi, tipicamente venivano filtrati grazie alle proprietà della descrizione a mezzo di BRIEF. Inoltre riducendo il tempo di campionamento tipicamente essi risultano quasi statici ed introducono un errore minore.
\end{itemize}

Al netto di quanto sperimentato si è compreso che:
\begin{enumerate}
	\item Il sistema di stima migliore è sicuramente quello di Kabsch, grazie alla sua proprietà naturali di filtraggio della misura.
	\item Si potrebbe investigare su qualche ulteriore algoritmo di stima in grado di gestire più corpi in movimento, così da evitare l'errore dovuto ad essi.
	\item Utilizzando punti chiave vicini si migliora la stima. Si potrebbe pensare ad una modalità di scarto di punti troppo lontani.
	\item La risoluzione dell'immagine è un grosso limite e si devo individuare il giusto compromesso tra accuratezza e prestazioni.
	\item Il sistema risulta più che promettente. Infatti, un errore di stima dell'ordine dei centesimi è più che sufficiente in molti contesti di controllo.
\end{enumerate}
